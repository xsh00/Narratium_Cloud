import { NodeTool } from "@/lib/nodeflow/NodeTool";
import { ChatOpenAI } from "@langchain/openai";
import { ChatOllama } from "@langchain/ollama";
import { ChatPromptTemplate } from "@langchain/core/prompts";
import { StringOutputParser } from "@langchain/core/output_parsers";
import { RunnablePassthrough } from "@langchain/core/runnables";

export interface LLMConfig {
  modelName: string;
  apiKey: string;
  baseUrl?: string;
  llmType: "openai" | "ollama";
  temperature?: number;
  maxTokens?: number;
  maxRetries?: number;
  topP?: number;
  frequencyPenalty?: number;
  presencePenalty?: number;
  topK?: number;
  repeatPenalty?: number;
  streaming?: boolean;
  streamUsage?: boolean;
  language?: "zh" | "en";
}
export class LLMNodeTools extends NodeTool {
  protected static readonly toolType: string = "llm";
  protected static readonly version: string = "1.0.0";

  static getToolType(): string {
    return this.toolType;
  }

  static async executeMethod(
    methodName: string,
    ...params: any[]
  ): Promise<any> {
    const method = (this as any)[methodName];

    if (typeof method !== "function") {
      console.error(
        `Method lookup failed: ${methodName} not found in LLMNodeTools`,
      );
      console.log(
        "Available methods:",
        Object.getOwnPropertyNames(this).filter(
          (name) =>
            typeof (this as any)[name] === "function" && !name.startsWith("_"),
        ),
      );
      throw new Error(
        `Method ${methodName} not found in ${this.getToolType()}Tool`,
      );
    }

    try {
      this.logExecution(methodName, params);
      return await (method as Function).apply(this, params);
    } catch (error) {
      console.error(`Method execution failed: ${methodName}`, error);
      throw error;
    }
  }

  static async invokeLLM(
    systemMessage: string,
    userMessage: string,
    config: LLMConfig,
  ): Promise<string> {
    const startTime = Date.now();
    console.log(`ðŸš€ [LLMæ€§èƒ½ç›‘æŽ§] å¼€å§‹LLMè°ƒç”¨ - æ—¶é—´: ${new Date().toISOString()}`);
    console.log(`ðŸ“ [LLMæ€§èƒ½ç›‘æŽ§] æ¨¡åž‹: ${config.modelName}, ç±»åž‹: ${config.llmType}`);
    console.log(`ðŸ“ [LLMæ€§èƒ½ç›‘æŽ§] ç³»ç»Ÿæ¶ˆæ¯é•¿åº¦: ${systemMessage.length} å­—ç¬¦`);
    console.log(`ðŸ“ [LLMæ€§èƒ½ç›‘æŽ§] ç”¨æˆ·æ¶ˆæ¯é•¿åº¦: ${userMessage.length} å­—ç¬¦`);
    
    try {
      // é˜¶æ®µ1: åˆ›å»ºLLMå®žä¾‹
      const stage1Start = Date.now();
      console.log(`ðŸ”„ [LLMæ€§èƒ½ç›‘æŽ§] é˜¶æ®µ1 - å¼€å§‹åˆ›å»ºLLMå®žä¾‹`);
      const llm = this.createLLM(config);
      const stage1End = Date.now();
      console.log(`âœ… [LLMæ€§èƒ½ç›‘æŽ§] é˜¶æ®µ1 - LLMå®žä¾‹åˆ›å»ºå®Œæˆ - è€—æ—¶: ${stage1End - stage1Start}ms`);

      // é˜¶æ®µ2: åˆ›å»ºå¯¹è¯é“¾
      const stage2Start = Date.now();
      console.log(`ðŸ”„ [LLMæ€§èƒ½ç›‘æŽ§] é˜¶æ®µ2 - å¼€å§‹åˆ›å»ºå¯¹è¯é“¾`);
      const dialogueChain = this.createDialogueChain(llm);
      const stage2End = Date.now();
      console.log(`âœ… [LLMæ€§èƒ½ç›‘æŽ§] é˜¶æ®µ2 - å¯¹è¯é“¾åˆ›å»ºå®Œæˆ - è€—æ—¶: ${stage2End - stage2Start}ms`);

      // é˜¶æ®µ3: æ‰§è¡ŒLLMè°ƒç”¨
      const stage3Start = Date.now();
      console.log(`ðŸ”„ [LLMæ€§èƒ½ç›‘æŽ§] é˜¶æ®µ3 - å¼€å§‹æ‰§è¡ŒLLMè°ƒç”¨`);
      const response = await dialogueChain.invoke({
        system_message: systemMessage,
        user_message: userMessage,
      });
      const stage3End = Date.now();
      console.log(`âœ… [LLMæ€§èƒ½ç›‘æŽ§] é˜¶æ®µ3 - LLMè°ƒç”¨å®Œæˆ - è€—æ—¶: ${stage3End - stage3Start}ms`);

      if (!response || typeof response !== "string") {
        throw new Error("Invalid response from LLM");
      }

      const totalTime = Date.now() - startTime;
      console.log(`ðŸŽ‰ [LLMæ€§èƒ½ç›‘æŽ§] LLMè°ƒç”¨å®Œæˆ - æ€»è€—æ—¶: ${totalTime}ms`);
      console.log(`ðŸ“Š [LLMæ€§èƒ½ç›‘æŽ§] å„é˜¶æ®µè€—æ—¶ç»Ÿè®¡:`);
      console.log(`   - é˜¶æ®µ1 (LLMåˆ›å»º): ${stage1End - stage1Start}ms`);
      console.log(`   - é˜¶æ®µ2 (å¯¹è¯é“¾åˆ›å»º): ${stage2End - stage2Start}ms`);
      console.log(`   - é˜¶æ®µ3 (LLMè°ƒç”¨): ${stage3End - stage3Start}ms`);
      console.log(`   - æ€»è€—æ—¶: ${totalTime}ms`);
      console.log(`ðŸ“ [LLMæ€§èƒ½ç›‘æŽ§] å“åº”é•¿åº¦: ${response.length} å­—ç¬¦`);

      return response;
    } catch (error) {
      const errorTime = Date.now() - startTime;
      console.error(`âŒ [LLMæ€§èƒ½ç›‘æŽ§] LLMè°ƒç”¨å¤±è´¥ - è€—æ—¶: ${errorTime}ms:`, error);
      this.handleError(error as Error, "invokeLLM");
    }
  }

  private static createLLM(config: LLMConfig): ChatOpenAI | ChatOllama {
    const safeModel = config.modelName?.trim() || "";
    const defaultSettings = {
      temperature: 0.7,
      maxTokens: undefined,
      timeout: 1000000000,
      maxRetries: 0,
      topP: 0.7,
      frequencyPenalty: 0,
      presencePenalty: 0,
      topK: 40,
      repeatPenalty: 1.1,
      streaming: true,
      streamUsage: false,
    };

    if (config.llmType === "openai") {
      return new ChatOpenAI({
        modelName: safeModel,
        openAIApiKey: config.apiKey,
        configuration: {
          baseURL: config.baseUrl?.trim() || undefined,
        },
        temperature: config.temperature ?? defaultSettings.temperature,
        maxRetries: config.maxRetries ?? defaultSettings.maxRetries,
        topP: config.topP ?? defaultSettings.topP,
        frequencyPenalty:
          config.frequencyPenalty ?? defaultSettings.frequencyPenalty,
        presencePenalty:
          config.presencePenalty ?? defaultSettings.presencePenalty,
        streaming: config.streaming ?? defaultSettings.streaming,
        streamUsage: config.streamUsage ?? defaultSettings.streamUsage,
      });
    } else if (config.llmType === "ollama") {
      return new ChatOllama({
        model: safeModel,
        baseUrl: config.baseUrl?.trim() || "http://localhost:11434",
        temperature: config.temperature ?? defaultSettings.temperature,
        topK: config.topK ?? defaultSettings.topK,
        topP: config.topP ?? defaultSettings.topP,
        frequencyPenalty:
          config.frequencyPenalty ?? defaultSettings.frequencyPenalty,
        presencePenalty:
          config.presencePenalty ?? defaultSettings.presencePenalty,
        repeatPenalty: config.repeatPenalty ?? defaultSettings.repeatPenalty,
        streaming: config.streaming ?? defaultSettings.streaming,
      });
    } else {
      throw new Error(`Unsupported LLM type: ${config.llmType}`);
    }
  }

  private static createDialogueChain(llm: ChatOpenAI | ChatOllama): any {
    const dialoguePrompt = ChatPromptTemplate.fromMessages([
      ["system", "{system_message}"],
      ["human", "{user_message}"],
    ]);

    return RunnablePassthrough.assign({
      system_message: (input: any) => input.system_message,
      user_message: (input: any) => input.user_message,
    })
      .pipe(dialoguePrompt)
      .pipe(llm)
      .pipe(new StringOutputParser());
  }
}
